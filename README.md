# n8n Documentation Scraper

Ein Python-basierter Web Scraper, der die komplette n8n Dokumentation mit der Jina AI Reader API scraped und in einer SQLite-Datenbank speichert.

## Features

- Verwendet Jina AI Reader API fÃ¼r sauberes Markdown-Format
- Rekursives Crawling aller verlinkten Seiten
- SQLite-Datenbank fÃ¼r strukturierte Speicherung
- Automatische Link-Extraktion und -Verfolgung
- Fortschrittsanzeige wÃ¤hrend des Crawlings
- Duplikat-Erkennung (keine doppelten URLs)
- Statistiken Ã¼ber gescrapte Daten

## Voraussetzungen

- Python 3.7+
- Jina AI API Key (bereits im Code eingetragen)
- Virtuelle Umgebung (empfohlen)

## Installation

1. Repository klonen oder Dateien herunterladen

2. Virtuelle Umgebung erstellen und aktivieren:
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac
```

3. AbhÃ¤ngigkeiten installieren:
```bash
pip install -r requirements.txt
```

## Verwendung

### Einfacher Start

Das Skript startet automatisch beim Integrations-Bereich und crawled alle verlinkten Seiten:

```bash
python scraper.py
```

### Konfiguration

Im Code kÃ¶nnen Sie folgende Parameter anpassen:

```python
# In scraper.py, main() Funktion:

# Start-URL Ã¤ndern
start_url = 'https://docs.n8n.io/integrations/'

# Maximale Anzahl Seiten begrenzen (fÃ¼r Tests)
scraper.crawl(start_url, max_pages=50)

# Unbegrenztes Crawling
scraper.crawl(start_url, max_pages=None)
```

## Datenbank-Schema

### Tabelle: `pages`
Speichert die gescrapten Seiten:
- `id`: PrimÃ¤rschlÃ¼ssel
- `url`: Eindeutige URL der Seite
- `title`: Seitentitel (extrahiert aus Markdown)
- `markdown_content`: VollstÃ¤ndiger Inhalt im Markdown-Format
- `scraped_at`: Zeitstempel des Scrapings
- `status`: Status (success/error)

### Tabelle: `links`
Speichert Links zwischen Seiten:
- `id`: PrimÃ¤rschlÃ¼ssel
- `source_url`: Quell-URL
- `target_url`: Ziel-URL
- `link_text`: Text des Links

## Ausgabe

WÃ¤hrend des Crawlings sehen Sie:
```
ğŸš€ Starte Crawling von: https://docs.n8n.io/integrations/
ğŸ“Š Max. Seiten: unbegrenzt

[1/âˆ] Crawling: https://docs.n8n.io/integrations/
  ğŸ“¥ Fetching via Jina AI: https://docs.n8n.io/integrations/
  âœ“ Gespeichert: Integrations
  ğŸ“ 45 Links gefunden, 45 in Warteschlange

[2/âˆ] Crawling: https://docs.n8n.io/integrations/builtin/
  ...
```

Nach Abschluss:
```
âœ… Crawling abgeschlossen!
ğŸ“Š Insgesamt 250 Seiten gecrawlt

ğŸ“ˆ Statistiken:
   âœ“ Erfolgreiche Seiten: 248
   âœ— Fehlerhafte Seiten: 2
   ğŸ”— Gespeicherte Links: 1250

ğŸ’¾ Datenbank geschlossen
```

## Datenbank-Abfragen

Nach dem Scraping kÃ¶nnen Sie die SQLite-Datenbank abfragen:

```python
import sqlite3

conn = sqlite3.connect('n8n_docs.db')
cursor = conn.cursor()

# Alle Seiten anzeigen
cursor.execute("SELECT title, url FROM pages WHERE status='success'")
pages = cursor.fetchall()

# Nach bestimmtem Inhalt suchen
cursor.execute("SELECT title, url FROM pages WHERE markdown_content LIKE '%webhook%'")
results = cursor.fetchall()

# Alle Links einer Seite
cursor.execute("SELECT target_url, link_text FROM links WHERE source_url = ?",
               ('https://docs.n8n.io/integrations/',))
links = cursor.fetchall()
```

Oder mit einem SQLite-Browser wie [DB Browser for SQLite](https://sqlitebrowser.org/).

## Features im Detail

### Jina AI Reader API
- Konvertiert HTML automatisch in sauberes Markdown
- Entfernt Navigation und Footer
- BehÃ¤lt die Struktur und Formatierung bei
- Extrahiert alle Links aus dem Content

### Intelligentes Crawling
- Vermeidet Duplikate durch URL-Tracking
- Filtert externe Links automatisch
- HÃ¶fliche VerzÃ¶gerung (1 Sekunde) zwischen Requests
- Kann jederzeit mit Ctrl+C abgebrochen werden

### Fehlerbehandlung
- Bei Fehlern wird die Seite als 'error' markiert
- Crawling wird fortgesetzt
- Statistiken zeigen erfolgreiche und fehlerhafte Seiten

## EinschrÃ¤nkungen

- Crawled nur Seiten von `docs.n8n.io`
- Ãœberspringt API-Endpoints, Downloads und Suchseiten
- PDF und ZIP-Dateien werden nicht heruntergeladen

## Anpassungen

### Andere Domains crawlen
```python
self.base_url = 'https://ihre-domain.de'
```

### Filter anpassen
In der `should_crawl()` Methode:
```python
exclude_patterns = [
    '/api/',
    '/downloads/',
    '.pdf',
    # Ihre eigenen AusschlÃ¼sse hier
]
```

### VerzÃ¶gerung Ã¤ndern
```python
time.sleep(1)  # In Sekunden
```

## Lizenz

MIT License - Frei verwendbar fÃ¼r private und kommerzielle Projekte

## Support

Bei Fragen oder Problemen erstellen Sie bitte ein Issue auf GitHub.
